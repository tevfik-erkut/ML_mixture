{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping demo\n",
    "## Tripadvisor hotel reviews\n",
    "### This notebook uses the Excel file *HotelsToScrap.xlsx* as the list of hotels to scrap\n",
    "#### Changes may be required due to Tripadvisor's continous updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages and do the initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow not verified SSL (Secure Socket Layer) certificates to be opened\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Firefox options (configurations)\n",
    "options = Options()\n",
    "\n",
    "# Add this argument to Options to avoid being detected as a robot\n",
    "options.add_argument(\"--disable-blink-features\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "\n",
    "# Add this argument to Options to hide Firefox (make it not visible)\n",
    "options.add_argument('--headless') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of the hotels to read the content\n",
    "hotelsToScrap = pd.read_excel(\"HotelsToScrap.xlsx\", sheet_name=\"Sheet1\", index_col=\"ID\", engine='openpyxl')\n",
    "hotelsToScrap.index = [\"Aloft_Miami\", \"Hampton Inn\", \"Myriad\", \"AvenidaPalace\", \"Envue\"]\n",
    "hotelsToScrap.URL = ['https://www.tripadvisor.co.uk/Hotel_Review-g34438-d10822168-Reviews-Aloft_Miami_Airport-Miami_Florida.html',\n",
    "       'https://www.tripadvisor.co.uk/Hotel_Review-g34792-d10746316-Reviews-Hampton_Inn_Suites_Braselton-Braselton_Georgia.html',\n",
    "       'https://www.tripadvisor.co.uk/Hotel_Review-g189158-d3323582-Reviews-Myriad_by_SANA_Hotels-Lisbon_Lisbon_District_Central_Portugal.html',\n",
    "       'https://www.tripadvisor.co.uk/Hotel_Review-g189158-d195643-Reviews-Hotel_Avenida_Palace-Lisbon_Lisbon_District_Central_Portugal.html',\n",
    "       'https://www.tripadvisor.co.uk/Hotel_Review-g46907-d17721483-Reviews-Envue_Autograph_Collection-Weehawken_New_Jersey.html']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for the resuls\n",
    "#review date ver qual e a specific variable\n",
    "hotelReviews = pd.DataFrame({'hotelID': pd.Series([], dtype='string'),\n",
    "                             'user': pd.Series([], dtype='string'),\n",
    "                             'rating': pd.Series([], dtype='float'),\n",
    "                             'text': pd.Series([], dtype='string'),\n",
    "                             'date': pd.Series([], dtype='string'), \n",
    "                             'location': pd.Series([], dtype='string')\n",
    "                             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to use in the Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open page and read HTML\n",
    "def openPageReadHTML(url):\n",
    "\n",
    "    # Open Firefox with Selenium\n",
    "    #binary = FirefoxBinary('/Applications/Firefox.app/Contents/MacOS/firefox-bin')  # in case of error, replace the Firexfox path with the one on your computer\n",
    "    #browser = webdriver.Firefox(firefox_binary=binary, options=options)\n",
    "    browser = webdriver.Safari(options=options)\n",
    "    browser.get(url)\n",
    "    time.sleep(1) # Wait one second\n",
    "\n",
    "    # If there is a privacy pop-up, click the OK button\n",
    "    privacy_button = browser.find_elements(By.ID,\"onetrust-accept-btn-handler\")\n",
    "    if len(privacy_button)>0:\n",
    "        browser.execute_script(\"arguments[0].click()\", privacy_button[0])\n",
    "        time.sleep(0.5) # Wait half a second\n",
    "\n",
    "    # Try to move into first review and click the button \n",
    "    # Some times it takes some time to load the page\n",
    "    clicked_button=False\n",
    "    while not clicked_button:\n",
    "        read_more_buttons = browser.find_elements(By.CLASS_NAME,\"eljVo\")\n",
    "        if len(read_more_buttons) > 0:\n",
    "            try: \n",
    "                browser.execute_script(\"arguments[0].scrollIntoView(true);\", read_more_buttons[0])\n",
    "                browser.execute_script(\"arguments[0].click()\", read_more_buttons[0])\n",
    "                time.sleep(0.5) # Wait half a second\n",
    "                clicked_button=True\n",
    "            except:\n",
    "                # Wait for one second to retry\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            # Wait for one second to retry\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Read the content close de browser\n",
    "    html_source = browser.page_source  \n",
    "    browser.quit()\n",
    "\n",
    "    # Transform the html into a BeautifulSoup object\n",
    "    soupObj = BeautifulSoup(html_source) \n",
    "\n",
    "    return soupObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each page\n",
    "def processPage(soupObj, hotelID, extractedDF):\n",
    "\n",
    "    # Read reviews\n",
    "    reviews = soupObj.find_all(\"div\", {\"data-test-target\": \"reviews-tab\"})\n",
    "    \n",
    "\n",
    "    # Get the list of reviews\n",
    "    reviewsList = reviews[0].select(\"div[class*=cWwQK]\")\n",
    "    # Loop thru each review\n",
    "    for i in range(0,len(reviewsList)):\n",
    "        # Get Rating\n",
    "        r1 = reviewsList[i].select(\"span[class*=ui_bubble_rating]\")\n",
    "        r2 = r1[0][\"class\"][1]\n",
    "        reviewRating = int(''.join(filter(str.isdigit, r2)))/10\n",
    "\n",
    "        # Get User\n",
    "        user = reviewsList[i].select(\"a[class*=ui_header_link]\")[0].string\n",
    "\n",
    "        # Get review text\n",
    "        t = reviewsList[i].select(\"q[class*=XllAv]\")[0]\n",
    "        reviewText = t.get_text()\n",
    "        \n",
    "        \n",
    "        # Get review date \n",
    "        reviewDate = reviewsList[i].select(\"div[class*=bcaHz]\")[0].get_text()\n",
    "        reviewDate = reviewDate.split()\n",
    "        reviewDate = reviewDate[-2] + \" \" + reviewDate[-1]\n",
    "        print(reviewDate)\n",
    "        \n",
    "        if reviewDate =='review Today':reviewDate = dt.datetime.today()\n",
    "       \n",
    "        elif re.search(r'[S]\\d{3}',reviewDate) == 'Sept':\n",
    "            reviewDate2 = reviewDate.replace('Sept', 'Sep')\n",
    "        \n",
    "        elif reviewDate =='review Yesterday':\n",
    "            reviewDate = dt.datetime.today()+dt.timedelta(days=-1)\n",
    "\n",
    "        elif re.search(r'[12]\\d{3}',reviewDate) is None:\n",
    "            reviewDate2 = reviewDate + ' ' + str(dt.datetime.today().year)\n",
    "            reviewDate = dt.datetime.strptime(reviewDate2, \"%d %b %Y\")\n",
    "\n",
    "        else:\n",
    "            reviewDate2 = '1 '+ reviewDate\n",
    "            reviewDate = dt.datetime.strptime(reviewDate2, \"%d %b %Y\")\n",
    "        \n",
    "        \n",
    "        # Print the date as a string\n",
    "        reviewDatestr = reviewDate.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        \n",
    "        \n",
    "        # Get location  \n",
    "        location = reviewsList[i].select(\"span[class*=fSiLz]\")\n",
    "        if len(location)>0:\n",
    "            location = location[0].get_text()\n",
    "        else:\n",
    "            location ='N/A'                               \n",
    "                            \n",
    "              \n",
    "        # Update extracted reviews dataframe\n",
    "        extractedDF = extractedDF.append({'hotelID': hotelID,\n",
    "                             'user': user,\n",
    "                             'rating': reviewRating,\n",
    "                             'text': reviewText,\n",
    "                             'date': reviewDate,\n",
    "                             'location': location\n",
    "                             }, ignore_index=True)\n",
    "\n",
    "    # Return the resulting dataframe\n",
    "    return extractedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = \"124.240.187.80:82\"\n",
    "\n",
    "webdriver.DesiredCapabilities.SAFARI['proxy'] = {\n",
    "   \"httpProxy\":proxy,\n",
    "   \"ftpProxy\":proxy,\n",
    "   \"sslProxy\":proxy,\n",
    "   \"noProxy\":None,\n",
    "   \"proxyType\":\"MANUAL\",\n",
    "   \"class\":\"org.openqa.selenium.Proxy\",\n",
    "   \"autodetect\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hotel Aloft_Miami\n",
      "May 2022\n",
      "May 2022\n",
      "May 2022\n",
      "May 2022\n",
      "May 2022\n",
      "Mar 2022\n",
      "Mar 2022\n",
      "Mar 2022\n",
      "Feb 2022\n",
      "Feb 2022\n",
      "Extracted  5 / 200\n",
      "Mar 2022\n",
      "Mar 2022\n",
      "Mar 2022\n",
      "Feb 2022\n",
      "Feb 2022\n",
      "Dec 2021\n",
      "Nov 2021\n",
      "Oct 2021\n",
      "Oct 2021\n",
      "Oct 2021\n",
      "Extracted  10 / 200\n",
      "Dec 2021\n",
      "Nov 2021\n",
      "Oct 2021\n",
      "Oct 2021\n",
      "Oct 2021\n",
      "Sep 2021\n",
      "Sep 2021\n",
      "Aug 2021\n",
      "Aug 2021\n",
      "Aug 2021\n",
      "Extracted  15 / 200\n",
      "Sep 2021\n",
      "Sep 2021\n",
      "Aug 2021\n",
      "Aug 2021\n",
      "Aug 2021\n",
      "Aug 2021\n",
      "Aug 2021\n",
      "Jul 2021\n",
      "Jul 2021\n",
      "Jul 2021\n",
      "Extracted  20 / 200\n",
      "Aug 2021\n",
      "Aug 2021\n",
      "Jul 2021\n",
      "Jul 2021\n",
      "Jul 2021\n",
      "Jul 2021\n",
      "May 2021\n",
      "Mar 2021\n",
      "Feb 2021\n",
      "Jan 2021\n",
      "Extracted  25 / 200\n",
      "Jul 2021\n",
      "May 2021\n",
      "Mar 2021\n",
      "Feb 2021\n",
      "Jan 2021\n",
      "Jan 2021\n",
      "Dec 2020\n",
      "Aug 2020\n",
      "Aug 2020\n",
      "Mar 2020\n",
      "Extracted  30 / 200\n",
      "Jan 2021\n",
      "Dec 2020\n",
      "Aug 2020\n",
      "Aug 2020\n",
      "Mar 2020\n",
      "Mar 2020\n",
      "Mar 2020\n",
      "Feb 2020\n",
      "Feb 2020\n",
      "Feb 2020\n",
      "Extracted  35 / 200\n",
      "Mar 2020\n",
      "Mar 2020\n",
      "Feb 2020\n",
      "Feb 2020\n",
      "Feb 2020\n",
      "Feb 2020\n",
      "Jan 2020\n",
      "Jan 2020\n",
      "Jan 2020\n",
      "Dec 2019\n",
      "Extracted  40 / 200\n",
      "Feb 2020\n",
      "Jan 2020\n",
      "Jan 2020\n",
      "Jan 2020\n",
      "Dec 2019\n",
      "Dec 2019\n",
      "Dec 2019\n",
      "Dec 2019\n",
      "Dec 2019\n",
      "Dec 2019\n",
      "Extracted  45 / 200\n",
      "Dec 2019\n",
      "Dec 2019\n",
      "Dec 2019\n",
      "Dec 2019\n",
      "Dec 2019\n",
      "Nov 2019\n",
      "Nov 2019\n",
      "Nov 2019\n",
      "Nov 2019\n",
      "Nov 2019\n",
      "Extracted  50 / 200\n",
      "Nov 2019\n",
      "Nov 2019\n",
      "Nov 2019\n",
      "Nov 2019\n",
      "Nov 2019\n",
      "Nov 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Extracted  55 / 200\n",
      "Nov 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Extracted  60 / 200\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Oct 2019\n",
      "Sep 2019\n",
      "Sep 2019\n",
      "Sep 2019\n",
      "Sep 2019\n",
      "Extracted  65 / 200\n",
      "Oct 2019\n",
      "Sep 2019\n",
      "Sep 2019\n",
      "Sep 2019\n",
      "Sep 2019\n",
      "Aug 2019\n",
      "Aug 2019\n",
      "Aug 2019\n",
      "Aug 2019\n",
      "Jul 2019\n",
      "Extracted  70 / 200\n",
      "Aug 2019\n",
      "Aug 2019\n",
      "Aug 2019\n",
      "Aug 2019\n",
      "Jul 2019\n",
      "Jul 2019\n",
      "Jul 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "Extracted  75 / 200\n",
      "Jul 2019\n",
      "Jul 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "May 2019\n",
      "Extracted  80 / 200\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "Jun 2019\n",
      "May 2019\n",
      "May 2019\n",
      "May 2019\n",
      "May 2019\n",
      "May 2019\n",
      "Apr 2019\n",
      "Extracted  85 / 200\n",
      "May 2019\n",
      "May 2019\n",
      "May 2019\n",
      "May 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Extracted  90 / 200\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Mar 2019\n",
      "Mar 2019\n",
      "Mar 2019\n",
      "Extracted  95 / 200\n",
      "Apr 2019\n",
      "Apr 2019\n",
      "Mar 2019\n",
      "Mar 2019\n",
      "Mar 2019\n",
      "Mar 2019\n",
      "Mar 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Extracted  100 / 200\n",
      "Mar 2019\n",
      "Mar 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Jan 2019\n",
      "Extracted  105 / 200\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Feb 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Extracted  110 / 200\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Extracted  115 / 200\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Jan 2019\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Extracted  120 / 200\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Nov 2018\n",
      "Extracted  125 / 200\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Dec 2018\n",
      "Nov 2018\n",
      "Nov 2018\n",
      "Nov 2018\n",
      "Nov 2018\n",
      "Nov 2018\n",
      "Nov 2018\n",
      "Extracted  130 / 200\n",
      "Nov 2018\n",
      "Nov 2018\n",
      "Nov 2018\n",
      "Nov 2018\n",
      "Nov 2018\n",
      "Oct 2018\n",
      "Oct 2018\n",
      "Oct 2018\n",
      "Oct 2018\n",
      "Oct 2018\n",
      "Extracted  135 / 200\n",
      "Oct 2018\n",
      "Oct 2018\n",
      "Oct 2018\n",
      "Oct 2018\n",
      "Oct 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Extracted  140 / 200\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Aug 2018\n",
      "Extracted  145 / 200\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Sep 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Extracted  150 / 200\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Jul 2018\n",
      "Extracted  155 / 200\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Aug 2018\n",
      "Jul 2018\n",
      "Jul 2018\n",
      "Jul 2018\n",
      "Jul 2018\n",
      "Jul 2018\n",
      "Jun 2018\n",
      "Extracted  160 / 200\n",
      "Jul 2018\n",
      "Jul 2018\n",
      "Jul 2018\n",
      "Jul 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Extracted  165 / 200\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "May 2018\n",
      "Apr 2018\n",
      "Extracted  170 / 200\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "Jun 2018\n",
      "May 2018\n",
      "Apr 2018\n",
      "Apr 2018\n",
      "Apr 2018\n",
      "Mar 2018\n",
      "Mar 2018\n",
      "Mar 2018\n",
      "Extracted  175 / 200\n",
      "Apr 2018\n",
      "Apr 2018\n",
      "Mar 2018\n",
      "Mar 2018\n",
      "Mar 2018\n",
      "Mar 2018\n",
      "Mar 2018\n",
      "Feb 2018\n",
      "Feb 2018\n",
      "Feb 2018\n",
      "Extracted  180 / 200\n",
      "Mar 2018\n",
      "Mar 2018\n",
      "Feb 2018\n",
      "Feb 2018\n",
      "Feb 2018\n",
      "Feb 2018\n",
      "Feb 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Extracted  185 / 200\n",
      "Feb 2018\n",
      "Feb 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Dec 2017\n",
      "Extracted  190 / 200\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Jan 2018\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Extracted  195 / 200\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Dec 2017\n",
      "Nov 2017\n",
      "Extracted  200 / 200\n"
     ]
    }
   ],
   "source": [
    "# Loop for all hotels\n",
    "for index, row in hotelsToScrap.iterrows():\n",
    "    if index == \"Aloft_Miami\":\n",
    "        reviewsToGet = 200\n",
    "    if index == \"Myriad\":\n",
    "        continue\n",
    "    if index == \"Hampton Inn\":\n",
    "        continue\n",
    "    if index == \"AvenidaPalace\":\n",
    "        continue\n",
    "    if index == \"Envue\":\n",
    "        continue\n",
    "    if index == \"Corinthia\":\n",
    "        continue\n",
    "    # Present feedback on which hotel is being processed\n",
    "    print(\"Processing hotel\", index)\n",
    "\n",
    "    # Reset counter per hotel\n",
    "    reviewsExtracted = 0    \n",
    "\n",
    "    # Loop until it extracts the pre-defined number of reviews\n",
    "    while reviewsExtracted<reviewsToGet:\n",
    "\n",
    "        # Define URL to use based on the number of reviews extracted so far\n",
    "        urlToUse = row['URL']\n",
    "        if reviewsExtracted>0:\n",
    "            repText = \"-Reviews-or\"+str(reviewsExtracted)+\"-\"\n",
    "            urlToUse = urlToUse.replace(\"-Reviews-\",repText)\n",
    "\n",
    "        # Open and read the web page content\n",
    "        soup = openPageReadHTML(urlToUse)\n",
    "\n",
    "        # Process web page\n",
    "        hotelReviews = processPage(soup, index, hotelReviews)\n",
    "\n",
    "        # Update counter\n",
    "        reviewsExtracted = reviewsExtracted + 5\n",
    "\n",
    "        # Present feedback on the number of extracted reviews\n",
    "        print(\"Extracted \",reviewsExtracted,\"/\",reviewsToGet)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotelReviews.to_csv(\"Aloft_Miami.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotelReviews.to_csv(\"HamptonInn.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotelReviews.to_csv(\"envue_full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using lambda function in order to convert the date in the extracted file.\n",
    "# Save the extracted reviews data frame to an Excel file\n",
    "hotelReviews['date'] = hotelReviews['date'].apply(lambda x: pd.to_datetime(x))\n",
    "hotelReviews['date'] = hotelReviews['date'].dt.date\n",
    "hotelReviews\n",
    "hotelReviews.to_excel(\"ExtractedReviewsTESTE2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheraton_reviews.to_csv(\"sheraton.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hotelID\n",
       "AvenidaPalace    2020-03-01\n",
       "Corinthia        2021-06-01\n",
       "Mar Mante        2021-06-01\n",
       "Myriad           2019-09-01\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotelReviews.groupby(\"hotelID\")[\"date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "myriad_reviews = hotelReviews.loc[hotelReviews.hotelID == \"AvenidaPalace \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "myriad_reviews.to_csv(\"myriad_reviews.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "avenida_palace_reviews = hotelReviews.loc[hotelReviews.hotelID == \"AvenidaPalace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "avenida_palace_reviews.to_csv(\"avenida_palace_reviews.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
